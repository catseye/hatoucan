#!/usr/bin/env python

import re
import sys

# Inefficient-but-public-domain Commodore BASIC 2.0 tokenizer.
# This work is in the public domain, covered under the UNLICENSE;
# see the file UNLICENSE in the root directory of this distribution,
# or http://www.unlicense.org/ for full details.

# references:
#   http://justsolve.archiveteam.org/wiki/Commodore_BASIC_tokenized_file
#   http://www.c64-wiki.com/index.php/BASIC_token

TOKENS = (
    ('END', 128),
    ('FOR', 129),
    ('NEXT', 130),
    ('DATA', 131),
    ('INPUT#', 132),
    ('INPUT', 133),
    ('DIM', 134),
    ('READ', 135),
    ('LET', 136),
    ('GOTO', 137),
    ('RUN', 138),
    ('IF', 139),
    ('RESTORE', 140),
    ('GOSUB', 141),
    ('RETURN', 142),
    ('REM', 143),
    ('STOP', 144),
    ('ON', 145),
    ('WAIT', 146),
    ('LOAD', 147),
    ('SAVE', 148),
    ('VERIFY', 149),
    ('DEF', 150),
    ('POKE', 151),
    ('PRINT#', 152),
    ('PRINT', 153),
    ('CONT', 154),
    ('LIST', 155),
    ('CLR', 156),
    ('CMD', 157),
    ('SYS', 158),
    ('OPEN', 159),
    ('CLOSE', 160),
    ('GET', 161),
    ('NEW', 162),
    ('TAB(', 163),
    ('TO', 164),
    ('FN', 165),
    ('SPC(', 166),
    ('THEN', 167),
    ('NOT', 168),
    ('STEP', 169),
    ('+', 170),
    ('*', 172),
    ('/', 173),
    ('^', 174),
    ('AND', 175),
    ('OR', 176),
    ('>', 177),
    ('=', 178),
    ('<', 179),
    ('SGN', 180),
    ('INT', 181),
    ('ABS', 182),
    ('USR', 183),
    ('FRE', 184),
    ('POS', 185),
    ('SQR', 186),
    ('RND', 187),
    ('LOG', 188),
    ('EXP', 189),
    ('COS', 190),
    ('SIN', 191),
    ('TAN', 192),
    ('ATN', 193),
    ('PEEK', 194),
    ('LEN', 195),
    ('STR$', 196),
    ('VAL', 197),
    ('ASC', 198),
    ('CHR$', 199),
    ('LEFT$', 200),
    ('RIGHT$', 201),
    ('MID$', 202),
    ('GO', 203),
)

LENGTH_SORTED_TOKENS = sorted(TOKENS, key=lambda pair: len(pair[0]), reverse=True)


def scan(s):
    # so inefficient.  I don't care.
    upped = s.upper()
    for (token, value) in LENGTH_SORTED_TOKENS:
        if upped.startswith(token):
           return (value, s[len(token):])
    return (ord(s[0]), s[1:])


def tokenize(s):
    # TODO: strip the line number off the front, return as an int
    bytes = []
    while s:
        (byte, s) = scan(s)
        bytes.append(byte)
    return (999, bytes)


class TokenizedLine(object):
    def __init__(self, s, addr):
        (line_number, bytes) = tokenize(s)
        self.line_number = line_number
        self.bytes = bytes
        self.addr = addr
        self.next_addr = None

    def __len__(self):
        return len(self.bytes) + 5

    def write_to(self, f):
        """f being a file-like object"""
        assert self.next_addr is not None
        # TODO: write line number in binary
        # TODO: write next_addr in binary
        for byte in self.bytes:
            f.write(chr(byte))
        f.write(chr(0))


def main(argv):
    # TODO: set sys.stdout to binary mode for Windows folks
    tokenized_lines = []
    start_addr = 0x0801
    addr = start_addr
    for line in sys.stdin:
        tokenized_line = TokenizedLine(line.rstrip(), addr)
        addr += len(tokenized_line)
        tokenized_lines.append(tokenized_line)

    i = 1
    while i < len(tokenized_lines):
        tokenized_lines[i - 1].next_addr = tokenized_lines[i].addr
        i += 1

    outfile = sys.stdout

    sys.write(outfile, chr(1) + chr(8))  # TODO: write helper for this

    for tokenized_line in tokenized_lines:
        tokenized_line.write_to(outfile)


if __name__ == '__main__':
    sys.exit(main(sys.argv[1:]))
